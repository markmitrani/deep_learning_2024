{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:49.282317600Z",
     "start_time": "2024-12-03T02:11:45.850318800Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install wget\n",
    "#%pip install torch\n",
    "import data_rnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.244317900Z",
     "start_time": "2024-12-03T02:11:49.283317300Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = data_rnn.load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.260319100Z",
     "start_time": "2024-12-03T02:11:50.247318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 19, 9, 379, 22, 11, 50, 52, 53, 290], 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0] # 0 is positive, 1 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.319323900Z",
     "start_time": "2024-12-03T02:11:50.262319200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i2w[word] for word in x_train[0]] # what is vocab?\n",
    "len(i2w) # what is vocab size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.329318Z",
     "start_time": "2024-12-03T02:11:50.278319100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2514 240.6318\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(l) for l in x_train]\n",
    "print(np.min(lengths), np.max(lengths), np.average(lengths)) # stats on sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.330325200Z",
     "start_time": "2024-12-03T02:11:50.308316900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i['.pad'], w2i['.start'], w2i['.end'], w2i['.unk'] # special tokens and their ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Padding and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.341318800Z",
     "start_time": "2024-12-03T02:11:50.324318400Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining a pad  function\n",
    "def pad(seq, pad_length):\n",
    "    padded = np.zeros(pad_length) # 0 is for padding\n",
    "    padded[0:len(seq)] = seq\n",
    "    return torch.tensor(padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.404317200Z",
     "start_time": "2024-12-03T02:11:50.338321400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad(x_train[0], 12).reshape(1,-1)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification, baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:16:47.311236200Z",
     "start_time": "2024-12-03T02:16:47.294237200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a tensor x, and return max across time dimension\n",
    "def MaxPoolTime(x):\n",
    "    return torch.amax(x, dim=1)\n",
    "\n",
    "class MlpModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super().__init__()\n",
    "        timestep = 12\n",
    "        numcls = 2\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        n_embeddings = len(i2w)\n",
    "        self.emb = torch.nn.Embedding(n_embeddings, embedding_size, padding_idx=0)\n",
    "        self.fc1 = torch.nn.Linear(embedding_size, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, numcls)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = MaxPoolTime(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.791317200Z",
     "start_time": "2024-12-03T02:11:50.417318200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\AppData\\Local\\Temp\\ipykernel_31972\\32988193.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3995, 0.6005]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "baseline_model = MlpModel()\n",
    "y = baseline_model.forward(padded)\n",
    "torch.nn.functional.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.780348200Z",
     "start_time": "2024-12-03T02:11:50.789317800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pad all\n",
    "padding_size = np.max(lengths) # how big should the padding be? --> max seq length\n",
    "\n",
    "# pad train and validation set\n",
    "padded_train = torch.stack([pad(x, padding_size) for x in x_train])\n",
    "padded_val = torch.stack([pad(x, padding_size) for x in x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 2514])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train.shape # check shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation datalaoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.796336800Z",
     "start_time": "2024-12-03T02:11:51.782318700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "# set to device\n",
    "padded_train = padded_train.to(device)\n",
    "padded_val = padded_val.to(device)\n",
    "y_train_tensor = torch.tensor(y_train).to(device)\n",
    "y_val_tensor = torch.tensor(y_val).to(device)\n",
    "\n",
    "# create train and val datasets with instance and label pairs\n",
    "train_dataset = TensorDataset(padded_train, torch.tensor(y_train))\n",
    "validation_dataset = TensorDataset(padded_val, torch.tensor(y_val))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader):\n",
    "    val_acc = 0\n",
    "    val_correct = 0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            instances, labels = data\n",
    "            fwd = model(instances)\n",
    "            predictions = torch.argmax(fwd, dim=1)\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute and return accuracy\n",
    "    val_acc = val_correct / total_samples\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:21:46.634744200Z",
     "start_time": "2024-12-03T02:16:51.509394100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, optimizer, nr_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(nr_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, labels = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(instances)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % len(trainloader)/len(batch)*2 == 0:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "                running_loss = 0.0\n",
    "        val_acc = validate(model, testloader)\n",
    "        print(f'Epoch {epoch}, validation acc.: {val_acc}')\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.88 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 2.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(baseline_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      2\u001b[0m baseline_model \u001b[38;5;241m=\u001b[39m MlpModel(batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, trainloader, testloader, optimizer, nr_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mMlpModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m MaxPoolTime(x)\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.88 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 2.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "baseline_model = MlpModel(batch_size).to(device)\n",
    "final_model = train_model(baseline_model, trainloader, testloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Writing your own Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only answers \"complete the missing parts\" question. The full implementation on the dataset is in q4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(torch.nn.Module):\n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(insize+hsize, hsize)\n",
    "        self.lin2 = torch.nn.Linear(hsize, outsize)\n",
    "\n",
    "    def forward(self, x, hidden=None): \n",
    "        '''\n",
    "        b: batch size\n",
    "        t: time steps (ie. sequence length)\n",
    "        e: dimension of each input vector\n",
    "        '''\n",
    "\n",
    "        b, t, e = x.size() \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float) #make a tensor of inputs (bxe) \n",
    "            \n",
    "        outs = []\n",
    "        for i in range(t): #iterate through each value of the sequence\n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1) #take only the value being iterated \n",
    "            inp = self.lin1(inp)\n",
    "            hidden = torch.nn.functional.sigmoid(inp)\n",
    "            out = self.lin2(hidden)\n",
    "            outs.append(out[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ElmanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        num_classes = 2\n",
    "        vocab_size = len(i2w)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc1 = Elman(embedding_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x[0])\n",
    "        x = torch.amax(x, dim=1)  # Max pooling across the time dimension\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        timesteps = 2514 # from np.max(lengths)\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        num_classes = 2\n",
    "        vocab_size = len(i2w)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc1 = torch.nn.RNN(timesteps, batch_size, embedding_size, batch_first=True)\n",
    "        self.fc2 = torch.nn.LTSM()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x[0])\n",
    "        x = torch.amax(x, dim=1)  # Max pooling across the time dimension\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Autoregressive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, (i2w, w2i) = data_rnn.load_ndfa(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, (i2w, w2i) = data_rnn.load_brackets(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10, 10],\n",
       " ['.pad',\n",
       "  '.start',\n",
       "  '.end',\n",
       "  '.unk',\n",
       "  '!',\n",
       "  'k',\n",
       "  'v',\n",
       "  'w',\n",
       "  'a',\n",
       "  'c',\n",
       "  's',\n",
       "  'l',\n",
       "  'm',\n",
       "  'b',\n",
       "  'u'],\n",
       " {'.pad': 0,\n",
       "  '.start': 1,\n",
       "  '.end': 2,\n",
       "  '.unk': 3,\n",
       "  '!': 4,\n",
       "  'k': 5,\n",
       "  'v': 6,\n",
       "  'w': 7,\n",
       "  'a': 8,\n",
       "  'c': 9,\n",
       "  's': 10,\n",
       "  'l': 11,\n",
       "  'm': 12,\n",
       "  'b': 13,\n",
       "  'u': 14},\n",
       " 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], i2w, w2i, len(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'a', 'b', 'c', '!', 'a', 'b', 'c', '!', 'a', 'b', 'c', '!', 's']\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [i2w[x] for x in x_train[100_000-2]]\n",
    "print(seq)\n",
    "print(len(seq))\n",
    "16-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding for the autoregressive task\n",
    "def pad_ar(seq, pad_length):\n",
    "    assert len(seq) <= pad_length-2, f\"pad length {pad_length} too short for sequence of length {len(seq)}\"\n",
    "\n",
    "    padded = np.zeros(pad_length) # 0 is for '.pad'\n",
    "    padded[0] = 1 # 1 is for '.start'\n",
    "    padded[1:len(seq)+1] = seq # insert sequence\n",
    "    padded[len(seq)+1] = 2 # 2 is for '.end'\n",
    "    \n",
    "    return torch.tensor(padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 13, 9, 4, 8, 13, 9, 4, 8, 13, 9, 4, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10,  8, 13,  9,  4,  8, 13,  9,  4,  8, 13,  9,  4, 10,  2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train[100_000-2])\n",
    "pad_ar(x_train[100_000-2], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(2), np.float64(13.96264), np.int64(158))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(x) for x in x_train]\n",
    "np.min(lengths), np.average(lengths), np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_size = np.max(lengths)+2 # accounts for start and end tokens\n",
    "padded_train = torch.stack([pad_ar(x, padding_size) for x in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 160])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_train), padded_train.shape\n",
    "padded_train[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150000, 159])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[:, 1:160].shape\n",
    "# torch.select(padded_train, 1, 1:-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifts tensor values by 1 to the left\n",
    "def create_target(tensor):\n",
    "    shifted = tensor[:, 1:tensor.shape[1]]\n",
    "    shifted = torch.cat((shifted, torch.zeros((shifted.shape[0], 1))), dim=1)\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = create_target(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert(all(padded_train[:,-1]) == 0)\n",
    "max(padded_train[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the last entry in shifted is always 0\n",
    "torch.unique(padded_test[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cat((shifted, torch.zeros(shifted.shape[0]).reshape(1,-1)), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super().__init__()\n",
    "        num_chars = len(i2w) # num chars given by i2w\n",
    "        embedding_size = 32\n",
    "        hidden = 16\n",
    "        n_embeddings = len(i2w)\n",
    "        self.emb = torch.nn.Embedding(num_chars, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden,\n",
    "                            num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden, num_chars)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        #print(f\"e {type(e)} shape: {e.shape}\")\n",
    "        h = self.lstm(e)[0]\n",
    "        #print(f\"h {type(h)} shape: {h.shape}\")\n",
    "        y = self.fc1(h)\n",
    "        #print(f\"y {type(y)} shape: {y.shape}\")\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# set to device\n",
    "padded_x = padded_train.to(device)\n",
    "padded_y = padded_test.to(device)\n",
    "\n",
    "# create train dataset with instance and label pairs\n",
    "train_dataset = TensorDataset(padded_x, padded_y)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, optimizer, nr_epochs = 3):\n",
    "    for epoch in range(nr_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, targets = batch\n",
    "            targets = targets.long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(instances)\n",
    "            outputs = torch.transpose(outputs, 1, 2)\n",
    "            #print(f'outputs: {outputs.shape}')\n",
    "            #print(f'targets: {targets.shape}')\n",
    "            #print(outputs.dtype)\n",
    "            #print(targets.dtype)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(trainloader)}\")\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 3.045847667709986\n",
      "Epoch 2, loss: 3.0458476652940116\n",
      "Epoch 3, loss: 3.0458476658821105\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "q6model = ARModel(batch_size=10).to(device)\n",
    "optimizer = optim.Adam(q6model.parameters(), lr=0.001)\n",
    "final_model = train_model(q6model, trainloader, optimizer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'('",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m seq \u001b[38;5;241m=\u001b[39m [w2i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.start\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mw2i\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, w2i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m], w2i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mKeyError\u001b[0m: '('"
     ]
    }
   ],
   "source": [
    "seq = [w2i['.start'], w2i['('], w2i['('], w2i[')']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
