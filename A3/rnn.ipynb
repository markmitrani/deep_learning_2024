{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:49.282317600Z",
     "start_time": "2024-12-03T02:11:45.850318800Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install wget\n",
    "#%pip install torch\n",
    "import data_rnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.244317900Z",
     "start_time": "2024-12-03T02:11:49.283317300Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = data_rnn.load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.260319100Z",
     "start_time": "2024-12-03T02:11:50.247318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 19, 9, 379, 22, 11, 50, 52, 53, 290], 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0] # 0 is positive, 1 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.319323900Z",
     "start_time": "2024-12-03T02:11:50.262319200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i2w[word] for word in x_train[0]] # what is vocab?\n",
    "len(i2w) # what is vocab size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.329318Z",
     "start_time": "2024-12-03T02:11:50.278319100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2514 240.6318\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(l) for l in x_train]\n",
    "print(np.min(lengths), np.max(lengths), np.average(lengths)) # stats on sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.330325200Z",
     "start_time": "2024-12-03T02:11:50.308316900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i['.pad'], w2i['.start'], w2i['.end'], w2i['.unk'] # special tokens and their ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Padding and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.341318800Z",
     "start_time": "2024-12-03T02:11:50.324318400Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining a pad  function\n",
    "def pad(seq, pad_length):\n",
    "    padded = np.zeros(pad_length) # 0 is for padding\n",
    "    padded[0:len(seq)] = seq\n",
    "    return torch.tensor(padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.404317200Z",
     "start_time": "2024-12-03T02:11:50.338321400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad(x_train[0], 12).reshape(1,-1)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification, baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:16:47.311236200Z",
     "start_time": "2024-12-03T02:16:47.294237200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a tensor x, and return max across time dimension\n",
    "def MaxPoolTime(x):\n",
    "    return torch.amax(x, dim=1)\n",
    "\n",
    "class MlpModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super().__init__()\n",
    "        timestep = 12\n",
    "        numcls = 2\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        n_embeddings = len(i2w)\n",
    "        self.emb = torch.nn.Embedding(n_embeddings, embedding_size, padding_idx=0)\n",
    "        self.fc1 = torch.nn.Linear(embedding_size, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, numcls)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = MaxPoolTime(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.791317200Z",
     "start_time": "2024-12-03T02:11:50.417318200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\AppData\\Local\\Temp\\ipykernel_31972\\32988193.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3995, 0.6005]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "baseline_model = MlpModel()\n",
    "y = baseline_model.forward(padded)\n",
    "torch.nn.functional.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.780348200Z",
     "start_time": "2024-12-03T02:11:50.789317800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pad all\n",
    "padding_size = np.max(lengths) # how big should the padding be? --> max seq length\n",
    "\n",
    "# pad train and validation set\n",
    "padded_train = torch.stack([pad(x, padding_size) for x in x_train])\n",
    "padded_val = torch.stack([pad(x, padding_size) for x in x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 2514])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train.shape # check shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation datalaoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.796336800Z",
     "start_time": "2024-12-03T02:11:51.782318700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "# set to device\n",
    "padded_train = padded_train.to(device)\n",
    "padded_val = padded_val.to(device)\n",
    "y_train_tensor = torch.tensor(y_train).to(device)\n",
    "y_val_tensor = torch.tensor(y_val).to(device)\n",
    "\n",
    "# create train and val datasets with instance and label pairs\n",
    "train_dataset = TensorDataset(padded_train, torch.tensor(y_train))\n",
    "validation_dataset = TensorDataset(padded_val, torch.tensor(y_val))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader):\n",
    "    val_acc = 0\n",
    "    val_correct = 0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            instances, labels = data\n",
    "            fwd = model(instances)\n",
    "            predictions = torch.argmax(fwd, dim=1)\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute and return accuracy\n",
    "    val_acc = val_correct / total_samples\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:21:46.634744200Z",
     "start_time": "2024-12-03T02:16:51.509394100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, optimizer, nr_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(nr_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, labels = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(instances)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % len(trainloader)/len(batch)*2 == 0:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "                running_loss = 0.0\n",
    "        val_acc = validate(model, testloader)\n",
    "        print(f'Epoch {epoch}, validation acc.: {val_acc}')\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "baseline_model = MlpModel(batch_size).to(device)\n",
    "final_model = train_model(baseline_model, trainloader, testloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Writing your own Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only answers \"complete the missing parts\" question. The full implementation on the dataset is in q4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(torch.nn.Module):\n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(insize+hsize, hsize)\n",
    "        self.lin2 = torch.nn.Linear(hsize, outsize)\n",
    "\n",
    "    def forward(self, x, hidden=None): \n",
    "        '''\n",
    "        b: batch size\n",
    "        t: time steps (ie. sequence length)\n",
    "        e: dimension of each input vector\n",
    "        '''\n",
    "\n",
    "        b, t, e = x.size() \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float) #make a tensor of inputs (bxe) \n",
    "            \n",
    "        outs = []\n",
    "        for i in range(t): #iterate through each value of the sequence\n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1) #take only the value being iterated \n",
    "            inp = self.lin1(inp)\n",
    "            hidden = torch.nn.functional.sigmoid(inp)\n",
    "            out = self.lin2(hidden)\n",
    "            outs.append(out[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ElmanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        num_classes = 2\n",
    "        vocab_size = len(i2w)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc1 = Elman(embedding_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x[0])\n",
    "        x = torch.amax(x, dim=1)  # Max pooling across the time dimension\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        timesteps = 2514 # from np.max(lengths)\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        num_classes = 2\n",
    "        vocab_size = len(i2w)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc1 = torch.nn.RNN(timesteps, batch_size, embedding_size, batch_first=True)\n",
    "        self.fc2 = torch.nn.LTSM()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x[0])\n",
    "        x = torch.amax(x, dim=1)  # Max pooling across the time dimension\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Autoregressive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, (i2w, w2i) = data_rnn.load_ndfa(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, (i2w, w2i) = data_rnn.load_brackets(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 4],\n",
       " ['.pad', '.start', '.end', '.unk', ')', '('],\n",
       " {'.pad': 0, '.start': 1, '.end': 2, '.unk': 3, ')': 4, '(': 5},\n",
       " 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], i2w, w2i, len(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', '(', ')', ')']\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [i2w[x] for x in x_train[100_000-2]]\n",
    "print(seq)\n",
    "print(len(seq))\n",
    "16-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding for the autoregressive task\n",
    "def pad_ar(seq, pad_length):\n",
    "    assert len(seq) <= pad_length-2, f\"pad length {pad_length} too short for sequence of length {len(seq)}\"\n",
    "\n",
    "    padded = np.zeros(pad_length) # 0 is for '.pad'\n",
    "    padded[0] = 1 # 1 is for '.start'\n",
    "    padded[1:len(seq)+1] = seq # insert sequence\n",
    "    padded[len(seq)+1] = 2 # 2 is for '.end'\n",
    "    \n",
    "    return torch.tensor(padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 5, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train[100_000-2])\n",
    "pad_ar(x_train[100_000-2], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(2), np.float64(9.01628), np.int64(1022))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(x) for x in x_train]\n",
    "np.min(lengths), np.average(lengths), np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_size = np.max(lengths)+2 # accounts for start and end tokens\n",
    "padded_train = torch.stack([pad_ar(x, padding_size) for x in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_train), padded_train.shape\n",
    "padded_train[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150000, 159])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[:, 1:160].shape\n",
    "# torch.select(padded_train, 1, 1:-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifts tensor values by 1 to the left\n",
    "def create_target(tensor):\n",
    "    shifted = tensor[:, 1:tensor.shape[1]]\n",
    "    shifted = torch.cat((shifted, torch.zeros((shifted.shape[0], 1))), dim=1)\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = create_target(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert(all(padded_train[:,-1]) == 0)\n",
    "max(padded_train[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the last entry in shifted is always 0\n",
    "torch.unique(padded_test[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cat((shifted, torch.zeros(shifted.shape[0]).reshape(1,-1)), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super().__init__()\n",
    "        num_chars = len(i2w) # num chars given by i2w\n",
    "        embedding_size = 32\n",
    "        hidden = 16\n",
    "        n_embeddings = len(i2w)\n",
    "        self.emb = torch.nn.Embedding(num_chars, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden,\n",
    "                            num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden, num_chars)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        #print(f\"e {type(e)} shape: {e.shape}\")\n",
    "        h = self.lstm(e)[0]\n",
    "        #print(f\"h {type(h)} shape: {h.shape}\")\n",
    "        y = self.fc1(h)\n",
    "        #print(f\"y {type(y)} shape: {y.shape}\")\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# set to device\n",
    "padded_x = padded_train.to(device)\n",
    "padded_y = padded_test.to(device)\n",
    "\n",
    "# create train dataset with instance and label pairs\n",
    "train_dataset = TensorDataset(padded_x, padded_y)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, optimizer, nr_epochs = 3):\n",
    "    for epoch in range(nr_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, targets = batch\n",
    "            targets = targets.long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(instances)\n",
    "            outputs = torch.transpose(outputs, 1, 2)\n",
    "            #print(f'outputs: {outputs.shape}')\n",
    "            #print(f'targets: {targets.shape}')\n",
    "            #print(outputs.dtype)\n",
    "            #print(targets.dtype)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(trainloader)}\")\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.010511511370442652\n",
      "Epoch 2, loss: 0.005372554556489922\n",
      "Epoch 3, loss: 0.0053703942185655855\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "q6model = ARModel(batch_size=10).to(device)\n",
    "optimizer = optim.Adam(q6model.parameters(), lr=0.001)\n",
    "final_model = train_model(q6model, trainloader, optimizer, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome logits\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given\n",
    "    distribution, 0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = torch.nn.functional.softmax(lnprobs / temperature, dim=1)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [w2i['.start'], w2i['('], w2i['('], w2i[')']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sequence(seq, model, temp = 1.0, max_len = 15):\n",
    "    seq_tensor = torch.tensor(seq).reshape(1,-1).to(device)\n",
    "    for i in range(max_len):\n",
    "        next_index = sample(model(seq_tensor).select(dim=1, index=-1), temp)\n",
    "        next_char = i2w[next_index]\n",
    "        if next_char == '.end':\n",
    "            print(next_char)\n",
    "            break\n",
    "        else:\n",
    "            print(next_char)\n",
    "            seq.append(next_index)\n",
    "            seq_tensor = torch.tensor(seq).reshape(1,-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      ")\n",
      ")\n",
      ".end\n"
     ]
    }
   ],
   "source": [
    "seq = [w2i['.start'], w2i['('], w2i['('], w2i[')']]\n",
    "complete_sequence(seq, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sampling(model, trainloader, optimizer, seq, nr_epochs = 3):\n",
    "    for epoch in range(nr_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, targets = batch\n",
    "            targets = targets.long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(instances)\n",
    "            outputs = torch.transpose(outputs, 1, 2)\n",
    "            #print(f'outputs: {outputs.shape}')\n",
    "            #print(f'targets: {targets.shape}')\n",
    "            #print(outputs.dtype)\n",
    "            #print(targets.dtype)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, loss: {running_loss/len(trainloader)}\")\n",
    "        print(\"Completion of sequence:\")\n",
    "        complete_sequence(seq, model)\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.0115712315647436\n",
      "Completion of sequence:\n",
      "(\n",
      ")\n",
      "(\n",
      "(\n",
      "(\n",
      "(\n",
      ")\n",
      "(\n",
      "(\n",
      ")\n",
      ")\n",
      ")\n",
      "(\n",
      "(\n",
      ")\n",
      "Epoch 2, loss: 0.005370287700827855\n",
      "Completion of sequence:\n",
      "(\n",
      ")\n",
      ")\n",
      "(\n",
      "(\n",
      "(\n",
      "(\n",
      ")\n",
      ")\n",
      "(\n",
      "(\n",
      ")\n",
      ")\n",
      "(\n",
      ")\n",
      "Epoch 3, loss: 0.005368025269561137\n",
      "Completion of sequence:\n",
      ")\n",
      ")\n",
      "(\n",
      ")\n",
      ")\n",
      "(\n",
      "(\n",
      ")\n",
      "(\n",
      ")\n",
      ")\n",
      ")\n",
      ")\n",
      ".end\n",
      "Epoch 4, loss: 0.005367565005715005\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 5, loss: 0.005367140282434411\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 6, loss: 0.005367292053023508\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 7, loss: 0.005367190484815122\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 8, loss: 0.005367088463766655\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 9, loss: 0.005367189712830198\n",
      "Completion of sequence:\n",
      ".end\n",
      "Epoch 10, loss: 0.0053668386728852055\n",
      "Completion of sequence:\n",
      ".end\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "seq = [w2i['.start'], w2i['('], w2i['('], w2i[')']]\n",
    "q6model = ARModel(batch_size=1).to(device)\n",
    "optimizer = optim.Adam(q6model.parameters(), lr=0.001)\n",
    "final_model = train_model_sampling(q6model, trainloader, optimizer, seq, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
