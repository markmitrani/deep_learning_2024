{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:33:02.994449700Z",
     "start_time": "2024-12-09T11:32:54.773986Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install wget\n",
    "#%pip install torch\n",
    "import data_rnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:33:04.554524700Z",
     "start_time": "2024-12-09T11:33:02.994449700Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = data_rnn.load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.260319100Z",
     "start_time": "2024-12-03T02:11:50.247318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 19, 9, 379, 22, 11, 50, 52, 53, 290], 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0] # 0 is positive, 1 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.319323900Z",
     "start_time": "2024-12-03T02:11:50.262319200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99430"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i2w[word] for word in x_train[0]] # what is vocab?\n",
    "len(i2w) # what is vocab size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.329318Z",
     "start_time": "2024-12-03T02:11:50.278319100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2514 240.6318\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(l) for l in x_train]\n",
    "print(np.min(lengths), np.max(lengths), np.average(lengths)) # stats on sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.330325200Z",
     "start_time": "2024-12-03T02:11:50.308316900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i['.pad'], w2i['.start'], w2i['.end'], w2i['.unk'] # special tokens and their ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Padding and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.341318800Z",
     "start_time": "2024-12-03T02:11:50.324318400Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining a pad  function\n",
    "def pad(seq, pad_length):\n",
    "    padded = np.zeros(pad_length) # 0 is for padding\n",
    "    padded[0:len(seq)] = seq\n",
    "    return torch.tensor(padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.404317200Z",
     "start_time": "2024-12-03T02:11:50.338321400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad(x_train[0], 12).reshape(1,-1)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification, baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:16:47.311236200Z",
     "start_time": "2024-12-03T02:16:47.294237200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a tensor x, and return max across time dimension\n",
    "def MaxPoolTime(x):\n",
    "    return torch.amax(x, dim=1)\n",
    "\n",
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super().__init__()\n",
    "        timestep = 12\n",
    "        numcls = 2\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        n_embeddings = len(i2w)\n",
    "        self.emb = torch.nn.Embedding(n_embeddings, embedding_size, padding_idx=0)\n",
    "        self.fc1 = torch.nn.Linear(embedding_size, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, numcls)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = MaxPoolTime(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:50.791317200Z",
     "start_time": "2024-12-03T02:11:50.417318200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zw/n1g26qmn72ldwxh52bkgncv40000gn/T/ipykernel_17660/3313671974.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5154, 0.4846]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "baseline_model = MLPModel()\n",
    "y = baseline_model.forward(padded)\n",
    "torch.nn.functional.softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.780348200Z",
     "start_time": "2024-12-03T02:11:50.789317800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pad all\n",
    "padding_size = np.max(lengths) # how big should the padding be? --> max seq length\n",
    "\n",
    "# pad train and validation set\n",
    "padded_train = torch.stack([pad(x, padding_size) for x in x_train])\n",
    "padded_val = torch.stack([pad(x, padding_size) for x in x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 2514])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train.shape # check shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation datalaoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:11:51.796336800Z",
     "start_time": "2024-12-03T02:11:51.782318700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "# set to device\n",
    "padded_train = padded_train.to(device)\n",
    "padded_val = padded_val.to(device)\n",
    "y_train_tensor = torch.tensor(y_train).to(device)\n",
    "y_val_tensor = torch.tensor(y_val).to(device)\n",
    "\n",
    "# create train and val datasets with instance and label pairs\n",
    "train_dataset = TensorDataset(padded_train, torch.tensor(y_train))\n",
    "validation_dataset = TensorDataset(padded_val, torch.tensor(y_val))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader):\n",
    "    val_acc = 0\n",
    "    val_correct = 0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            instances, labels = data\n",
    "            fwd = model(instances)\n",
    "            predictions = torch.argmax(fwd, dim=1)\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute and return accuracy\n",
    "    val_acc = val_correct / total_samples\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T02:21:46.634744200Z",
     "start_time": "2024-12-03T02:16:51.509394100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.713\n",
      "[1,   400] loss: 0.708\n",
      "[1,   600] loss: 0.713\n",
      "Epoch 0, validation acc.: 0.4926\n",
      "[2,   200] loss: 0.714\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, trainloader, testloader, optimizer, nr_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(nr_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            instances, labels = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(instances)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % len(trainloader)/len(batch)*2 == 0:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "                running_loss = 0.0\n",
    "        val_acc = validate(model, testloader)\n",
    "        print(f'Epoch {epoch}, validation acc.: {val_acc}')\n",
    "    print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "baseline_model = MLPModel(batch_size).to(device)\n",
    "final_model = train_model(baseline_model, trainloader, testloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Writing your own Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only answers \"complete the missing parts\" question. The full implementation on the dataset is in q4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(torch.nn.Module):\n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(insize+hsize, hsize)\n",
    "        self.lin2 = torch.nn.Linear(hsize, outsize)\n",
    "\n",
    "    def forward(self, x, hidden=None): \n",
    "        '''\n",
    "        b: batch size\n",
    "        t: time steps (ie. sequence length)\n",
    "        e: dimension of each input vector\n",
    "        '''\n",
    "\n",
    "        b, t, e = x.size() \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float) #make a tensor of inputs (bxe) \n",
    "            \n",
    "        outs = []\n",
    "        for i in range(t): #iterate through each value of the sequence\n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1) #take only the value being iterated \n",
    "            inp = self.lin1(inp)\n",
    "            hidden = torch.nn.functional.sigmoid(inp)\n",
    "            out = self.lin2(hidden)\n",
    "            outs.append(out[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ElmanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        hidden = 300\n",
    "        embedding_size = 300\n",
    "        num_classes = 2\n",
    "        vocab_size = len(i2w)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc1 = Elman(embedding_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x[0])\n",
    "        x = torch.amax(x, dim=1)  # Max pooling across the time dimension\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from question5.hypertuning import main as run_hypertuning\n",
    "\n",
    "run_hypertuning()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
